import os
import sys
import json
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
import torchvision.io as io
from torch.cuda.amp import autocast, GradScaler
from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts
from tqdm.auto import tqdm
import numpy as np
import warnings

# è­¦å‘Šã‚’ç„¡è¦–
warnings.filterwarnings("ignore", category=FutureWarning)

sys.path.append('/home/ollo/VideoMAE/videomae-clean')
sys.path.append('/home/ollo/VideoMAE')
sys.path.append('/home/ollo/VideoMAE/videomae-clean/MAE6_7')
sys.path.append('/home/ollo/VideoMAE/AVION')

from AVION.avion.models.model_videomae import VisionTransformer
from mixup import Mixup
import volume_transforms as volume_transforms

class Compose:
    def __init__(self, transforms):
        self.transforms = transforms
    
    def __call__(self, clip):
        for t in self.transforms:
            clip = t(clip)
        return clip

volume_transforms.Compose = Compose

class ImprovedEgo4DFlashDataset(Dataset):
    def __init__(self, annotation_file, video_root, transform=None, num_frames=16, num_classes=58, is_training=True):
        with open(annotation_file, "r") as f:
            self.annotations = json.load(f)["annotations"]
        self.video_root = video_root
        self.transform = transform
        self.num_frames = num_frames
        self.num_classes = num_classes
        self.is_training = is_training
    
    def random_crop(self, video, crop_size=(224, 224)):
        """ãƒ“ãƒ‡ã‚ªã«ãƒ©ãƒ³ãƒ€ãƒ ã‚¯ãƒ­ãƒƒãƒ—ã‚’é©ç”¨"""
        _, _, h, w = video.shape
        crop_h, crop_w = crop_size
        
        if h <= crop_h and w <= crop_w:
            return video
        
        top = torch.randint(0, h - crop_h + 1, (1,)).item() if h > crop_h else 0
        left = torch.randint(0, w - crop_w + 1, (1,)).item() if w > crop_w else 0
        
        return video[:, :, top:top+crop_h, left:left+crop_w]
    
    def color_jitter(self, video, brightness=0.2, contrast=0.2):
        """ç°¡å˜ãªè‰²èª¿æ•´"""
        if torch.rand(1) < 0.5:  # 50%ã®ç¢ºç‡ã§é©ç”¨
            # æ˜åº¦èª¿æ•´
            brightness_factor = 1.0 + (torch.rand(1) - 0.5) * brightness
            video = video * brightness_factor
            
            # ã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆèª¿æ•´
            contrast_factor = 1.0 + (torch.rand(1) - 0.5) * contrast
            mean = video.mean(dim=(2, 3), keepdim=True)
            video = (video - mean) * contrast_factor + mean
            
            # 0-1ã®ç¯„å›²ã«ã‚¯ãƒªãƒƒãƒ—
            video = torch.clamp(video, 0, 1)
        
        return video
    
    def __len__(self):
        return len(self.annotations)
    
    def __getitem__(self, idx):
        ann = self.annotations[idx]
        video_path = os.path.join(self.video_root, ann["video_url"])
        
        try:
            video, _, _ = io.read_video(video_path, pts_unit='sec')
        except Exception as e:
            print(f"ãƒ“ãƒ‡ã‚ªèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {video_path}, {e}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿
            video = torch.zeros(self.num_frames, 224, 224, 3)
        
        T = video.shape[0]
        
        # ã‚ˆã‚Šè‰¯ã„ãƒ•ãƒ¬ãƒ¼ãƒ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
        if T < self.num_frames:
            # ãƒ•ãƒ¬ãƒ¼ãƒ æ•°ãŒä¸è¶³ã—ã¦ã„ã‚‹å ´åˆã®è£œé–“
            indices = torch.linspace(0, T - 1, self.num_frames).long()
            video = video[indices]
        else:
            if self.is_training:
                # ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æ™‚ï¼‰
                start_idx = torch.randint(0, max(1, T - self.num_frames), (1,)).item()
                indices = torch.arange(start_idx, start_idx + self.num_frames)
            else:
                # ç­‰é–“éš”ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆæ¤œè¨¼æ™‚ï¼‰
                indices = torch.linspace(0, T - 1, self.num_frames).long()
            video = video[indices]
        
        # å½¢çŠ¶ã‚’èª¿æ•´
        video = video.permute(0, 3, 1, 2).float() / 255.0
        
        # ã‚«ã‚¹ã‚¿ãƒ ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µï¼ˆãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æ™‚ã®ã¿ï¼‰
        if self.is_training:
            # ã‚ˆã‚Šå¤§ããªã‚µã‚¤ã‚ºã«ãƒªã‚µã‚¤ã‚ºã—ã¦ã‹ã‚‰ãƒ©ãƒ³ãƒ€ãƒ ã‚¯ãƒ­ãƒƒãƒ—
            if video.shape[-2:] != (256, 256):
                video = torch.nn.functional.interpolate(
                    video, size=(256, 256), mode='bilinear', align_corners=False
                )
            video = self.random_crop(video, (224, 224))
            
            # è‰²èª¿æ•´
            video = self.color_jitter(video)
        else:
            # æ¤œè¨¼æ™‚ã¯å˜ç´”ã«ãƒªã‚µã‚¤ã‚º
            if video.shape[-2:] != (224, 224):
                video = torch.nn.functional.interpolate(
                    video, size=(224, 224), mode='bilinear', align_corners=False
                )
        
        # volume_transformsã‚’é©ç”¨
        if self.transform:
            video = self.transform(video)
        
        video = video.permute(1, 0, 2, 3)  # (C, T, H, W)
        
        label = ann["label"]
        if not isinstance(label, list):
            label = [label]
        
        target = torch.zeros(self.num_classes)
        for l in label:
            if 0 <= l < self.num_classes:
                target[l] = 1.0
        
        return video, target

def compute_precision_recall_f1_per_class(y_pred, y_target):
    """ã‚¯ãƒ©ã‚¹åˆ¥ã®è©³ç´°ãªè©•ä¾¡æŒ‡æ¨™ã‚’è¨ˆç®—"""
    num_classes = y_pred.shape[1]
    precisions, recalls, f1s = [], [], []
    
    for i in range(num_classes):
        tp = np.sum(y_pred[:, i] * y_target[:, i])
        fp = np.sum(y_pred[:, i] * (1 - y_target[:, i]))
        fn = np.sum((1 - y_pred[:, i]) * y_target[:, i])
        
        precision = tp / (tp + fp) if (tp + fp) > 0 else 0
        recall = tp / (tp + fn) if (tp + fn) > 0 else 0
        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 1e-4 else 0
        
        precisions.append(precision)
        recalls.append(recall)
        f1s.append(f1)
    
    return np.mean(precisions), np.mean(recalls), np.mean(f1s)

def convert_checkpoint_keys(checkpoint_dict):
    """ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã®ã‚­ãƒ¼ã‚’å¤‰æ›ã—ã¦ãƒ¢ãƒ‡ãƒ«ã¨ä¸€è‡´ã•ã›ã‚‹"""
    new_dict = {}
    
    for key, value in checkpoint_dict.items():
        # module.visual.* ã‹ã‚‰ visual.* ã¸ã®å¤‰æ›
        if key.startswith('module.visual.'):
            new_key = key.replace('module.visual.', '')
            new_dict[new_key] = value
        # module.* ã‹ã‚‰ * ã¸ã®å¤‰æ›
        elif key.startswith('module.'):
            new_key = key.replace('module.', '')
            new_dict[new_key] = value
        else:
            new_dict[key] = value
    
    return new_dict

def load_checkpoint_with_key_matching(model, checkpoint_path, device):
    """ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ãƒ­ãƒ¼ãƒ‰ã—ã€ã‚­ãƒ¼ã®ä¸ä¸€è‡´ã‚’å‡¦ç†ã™ã‚‹"""
    if not os.path.exists(checkpoint_path):
        raise FileNotFoundError(f"Checkpoint not found: {checkpoint_path}")
    
    print(f"ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’èª­ã¿è¾¼ã¿ä¸­: {checkpoint_path}")
    ckpt_raw = torch.load(checkpoint_path, map_location=device)
    
    # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã®æ§‹é€ ã‚’ç¢ºèª
    if 'state_dict' in ckpt_raw:
        ckpt = ckpt_raw['state_dict']
        print("'state_dict'ã‚­ãƒ¼ã‹ã‚‰ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿")
    elif 'model' in ckpt_raw:
        ckpt = ckpt_raw['model']
        print("'model'ã‚­ãƒ¼ã‹ã‚‰ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿")
    else:
        ckpt = ckpt_raw
        print("ãƒ«ãƒ¼ãƒˆãƒ¬ãƒ™ãƒ«ã‹ã‚‰ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿")
    
    # ã‚­ãƒ¼å¤‰æ›
    print("ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã®ã‚­ãƒ¼ã‚’å¤‰æ›ä¸­...")
    ckpt_converted = convert_checkpoint_keys(ckpt)
    
    model_keys = set(model.state_dict().keys())
    ckpt_keys = set(ckpt_converted.keys())
    
    print(f"ãƒ¢ãƒ‡ãƒ«ã®ã‚­ãƒ¼æ•°: {len(model_keys)}")
    print(f"å¤‰æ›å¾Œãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã®ã‚­ãƒ¼æ•°: {len(ckpt_keys)}")
    
    # å…±é€šã™ã‚‹ã‚­ãƒ¼ã‚’è¦‹ã¤ã‘ã‚‹
    common_keys = model_keys.intersection(ckpt_keys)
    missing_keys = model_keys - ckpt_keys
    unexpected_keys = ckpt_keys - model_keys
    
    print(f"å…±é€šã‚­ãƒ¼æ•°: {len(common_keys)}")
    print(f"ä¸è¶³ã‚­ãƒ¼æ•°: {len(missing_keys)}")
    print(f"ä½™åˆ†ã‚­ãƒ¼æ•°: {len(unexpected_keys)}")
    
    if len(common_keys) > 0:
        print("ä¸€è‡´ã™ã‚‹ã‚­ãƒ¼ã®ä¾‹ï¼ˆæœ€åˆã®5å€‹ï¼‰:")
        for key in list(common_keys)[:5]:
            print(f"  âœ“ {key}")
    
    if len(missing_keys) > 0:
        print("ä¸è¶³ã—ã¦ã„ã‚‹ã‚­ãƒ¼ï¼ˆæœ€åˆã®5å€‹ï¼‰:")
        for key in list(missing_keys)[:5]:
            print(f"  - {key}")
    
    if len(unexpected_keys) > 0:
        print("ä½™åˆ†ãªã‚­ãƒ¼ï¼ˆæœ€åˆã®5å€‹ï¼‰:")
        for key in list(unexpected_keys)[:5]:
            print(f"  + {key}")
    
    # å…±é€šã™ã‚‹ã‚­ãƒ¼ã®ã¿ã‚’ä½¿ç”¨ã—ã¦ãƒ­ãƒ¼ãƒ‰
    filtered_ckpt = {k: v for k, v in ckpt_converted.items() if k in common_keys}
    
    # strict=Falseã§ãƒ­ãƒ¼ãƒ‰ï¼ˆä¸è¶³ã™ã‚‹ã‚­ãƒ¼ã¯ç„¡è¦–ï¼‰
    missing_keys_load, unexpected_keys_load = model.load_state_dict(filtered_ckpt, strict=False)
    
    print(f"âœ… ãƒ­ãƒ¼ãƒ‰å®Œäº†: {len(filtered_ckpt)}å€‹ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¾ã—ãŸ")
    if missing_keys_load:
        print(f"âš ï¸  åˆæœŸåŒ–ã•ã‚ŒãŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {len(missing_keys_load)}å€‹")
    
    # ãƒ­ãƒ¼ãƒ‰æˆåŠŸç‡ã‚’è¨ˆç®—
    success_rate = len(filtered_ckpt) / len(model_keys) * 100
    print(f"ğŸ“Š ãƒ­ãƒ¼ãƒ‰æˆåŠŸç‡: {success_rate:.1f}%")
    
    return len(filtered_ckpt) > 0

class FocalLoss(nn.Module):
    """ä¸å‡è¡¡ãƒ‡ãƒ¼ã‚¿ã«åŠ¹æœçš„ãªFocal Loss"""
    def __init__(self, alpha=1, gamma=2, reduction='mean'):
        super(FocalLoss, self).__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.reduction = reduction
    
    def forward(self, inputs, targets):
        bce_loss = nn.functional.binary_cross_entropy_with_logits(inputs, targets, reduction='none')
        pt = torch.exp(-bce_loss)
        focal_loss = self.alpha * (1 - pt) ** self.gamma * bce_loss
        
        if self.reduction == 'mean':
            return focal_loss.mean()
        elif self.reduction == 'sum':
            return focal_loss.sum()
        else:
            return focal_loss

def main():
    annotation_dir = "/home/ollo/VideoMAE/videomae-clean"
    video_root = "/srv/shared/data/ego4d/short_clips/verb_annotation_simple"
    converted_ckpt_path = "/home/ollo/VideoMAE/checkpoints/avion_finetune_cls_lavila_vitb_best_converted.pt"
    train_json = os.path.join(annotation_dir, "20250512_annotations_train.json")
    val_json = os.path.join(annotation_dir, "20250512_annotations_val.json")
    
    # åˆ©ç”¨å¯èƒ½ãªå¤‰æ›ã®ã¿ã‚’ä½¿ç”¨
    train_transform = volume_transforms.Compose([
        volume_transforms.Resize((224, 224)),
        volume_transforms.RandomHorizontalFlip(p=0.5),
        volume_transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ])
    
    val_transform = volume_transforms.Compose([
        volume_transforms.Resize((224, 224)),
        volume_transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ])
    
    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã®ä½œæˆ
    train_dataset = ImprovedEgo4DFlashDataset(train_json, video_root, train_transform, is_training=True)
    val_dataset = ImprovedEgo4DFlashDataset(val_json, video_root, val_transform, is_training=False)
    
    # ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’èª¿æ•´
    train_loader = DataLoader(train_dataset, batch_size=6, shuffle=True, num_workers=4, pin_memory=True, drop_last=True)
    val_loader = DataLoader(val_dataset, batch_size=6, shuffle=False, num_workers=4, pin_memory=True)
    
    # ãƒ‡ãƒã‚¤ã‚¹è¨­å®š
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: {device}")
    
    # ãƒ¢ãƒ‡ãƒ«ã®ä½œæˆ
    try:
        model = VisionTransformer(
            num_classes=58, 
            use_flash_attn=True, 
            drop_path_rate=0.3  # ãƒ‰ãƒ­ãƒƒãƒ—ãƒ‘ã‚¹ç‡ã‚’ä¸Šã’ã¦éå­¦ç¿’ã‚’é˜²ã
        ).to(device)
        print("âœ… ãƒ¢ãƒ‡ãƒ«ä½œæˆå®Œäº†")
    except Exception as e:
        print(f"âš ï¸  Flash Attentionã§ãƒ¢ãƒ‡ãƒ«ä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
        model = VisionTransformer(
            num_classes=58, 
            use_flash_attn=False, 
            drop_path_rate=0.3
        ).to(device)
        print("âœ… ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ¢ãƒ‡ãƒ«ã§ä½œæˆå®Œäº†")
    
    # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã®ãƒ­ãƒ¼ãƒ‰ï¼ˆä¿®æ­£ç‰ˆï¼‰
    try:
        success = load_checkpoint_with_key_matching(model, converted_ckpt_path, device)
        if success:
            print("ğŸ‰ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã®ãƒ­ãƒ¼ãƒ‰ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
        else:
            print("âš ï¸  è­¦å‘Š: ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ãƒ­ãƒ¼ãƒ‰ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚ãƒ©ãƒ³ãƒ€ãƒ åˆæœŸåŒ–ã§ç¶šè¡Œã—ã¾ã™ã€‚")
    except Exception as e:
        print(f"âŒ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")
        print("ãƒ©ãƒ³ãƒ€ãƒ åˆæœŸåŒ–ã§ç¶šè¡Œã—ã¾ã™ã€‚")
    
    # ã‚ˆã‚ŠåŠ¹æœçš„ãªMixupè¨­å®š
    mixup_fn = Mixup(
        mixup_alpha=0.4,  # ã‚ˆã‚Šæ§ãˆã‚ãªMixup
        cutmix_alpha=0.8, 
        prob=0.8,  # ç¢ºç‡ã‚’ä¸‹ã’ã¦ã€é€šå¸¸ã®å­¦ç¿’ã‚‚æ··ãœã‚‹
        switch_prob=0.5, 
        mode='batch', 
        label_smoothing=0.05,  # ãƒ©ãƒ™ãƒ«ã‚¹ãƒ ãƒ¼ã‚¸ãƒ³ã‚°ã‚’æ§ãˆã‚ã«
        num_classes=58
    )
    
    # ã‚ˆã‚ŠåŠ¹æœçš„ãªæœ€é©åŒ–è¨­å®š
    optimizer = optim.AdamW(
        model.parameters(), 
        lr=5e-4,  # å­¦ç¿’ç‡ã‚’ä¸‹ã’ã¦å®‰å®šæ€§å‘ä¸Š
        betas=(0.9, 0.999), 
        weight_decay=0.1,  # é‡ã¿æ¸›è¡°ã‚’å¼·ã‚ã¦æ­£å‰‡åŒ–
        eps=1e-8
    )
    
    # Cosine Annealing with Warm Restartsã‚’ä½¿ç”¨
    scheduler = CosineAnnealingWarmRestarts(
        optimizer, 
        T_0=10,  # 10ã‚¨ãƒãƒƒã‚¯ã§ãƒªã‚¹ã‚¿ãƒ¼ãƒˆ
        T_mult=1,
        eta_min=1e-6
    )
    
    # Focal Lossã‚’ä½¿ç”¨ï¼ˆä¸å‡è¡¡ãƒ‡ãƒ¼ã‚¿ã«åŠ¹æœçš„ï¼‰
    criterion = FocalLoss(alpha=1, gamma=2)
    scaler = GradScaler()
    
    # ã‚ˆã‚Šå¤šãã®ã‚¨ãƒãƒƒã‚¯æ•°
    num_epochs = 10  # ã‚¨ãƒãƒƒã‚¯æ•°ã‚’å¢—ã‚„ã™
    best_f1 = 0.0
    best_model_state = None
    
    print(f"ğŸš€ ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°é–‹å§‹: {num_epochs}ã‚¨ãƒãƒƒã‚¯")
    
    for epoch in range(num_epochs):
        # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ•ã‚§ãƒ¼ã‚º
        model.train()
        total_loss = 0.0
        num_batches = 0
        
        print(f"\nğŸ“ˆ ã‚¨ãƒãƒƒã‚¯ {epoch+1}/{num_epochs} é–‹å§‹")
        
        for videos, targets in tqdm(train_loader, desc=f"Epoch {epoch+1} Training"):
            try:
                videos, targets = videos.to(device), targets.to(device)
                
                # Mixupã‚’æ®µéšçš„ã«é©ç”¨ï¼ˆåˆæœŸã‚¨ãƒãƒƒã‚¯ã¯æ§ãˆã‚ã«ï¼‰
                if epoch >= 2:  # 2ã‚¨ãƒãƒƒã‚¯ç›®ã‹ã‚‰é©ç”¨
                    videos, targets = mixup_fn(videos, targets)
                
                optimizer.zero_grad()
                
                with autocast():
                    outputs = model(videos)
                    loss = criterion(outputs, targets)
                
                scaler.scale(loss).backward()
                # ã‚°ãƒ©ãƒ‡ã‚£ã‚¨ãƒ³ãƒˆã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°
                scaler.unscale_(optimizer)
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                
                scaler.step(optimizer)
                scaler.update()
                
                total_loss += loss.item()
                num_batches += 1
                
            except Exception as e:
                print(f"âš ï¸  ãƒãƒƒãƒå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
                continue
        
        scheduler.step()
        avg_train_loss = total_loss / max(num_batches, 1)
        print(f"ğŸ“Š [Epoch {epoch+1}] Training Loss: {avg_train_loss:.4f}")
        
        # æ¤œè¨¼ãƒ•ã‚§ãƒ¼ã‚º
        model.eval()
        all_preds, all_targets = [], []
        
        with torch.no_grad():
            for videos, targets in tqdm(val_loader, desc=f"Epoch {epoch+1} Validation"):
                try:
                    videos, targets = videos.to(device), targets.to(device)
                    
                    with autocast():
                        outputs = model(videos)
                    
                    # ã‚ˆã‚Šè‰¯ã„é–¾å€¤è¨­å®šï¼ˆè¤‡æ•°ã®é–¾å€¤ã§ãƒ†ã‚¹ãƒˆï¼‰
                    thresholds = [0.3, 0.4, 0.5]
                    best_threshold_f1 = 0
                    best_preds = None
                    
                    for threshold in thresholds:
                        temp_preds = (torch.sigmoid(outputs) > threshold).float()
                        temp_preds_np = temp_preds.cpu().numpy()
                        temp_targets_np = targets.cpu().numpy()
                        
                        # ä¸€æ™‚çš„ãªF1ã‚’è¨ˆç®—
                        _, _, temp_f1 = compute_precision_recall_f1_per_class(temp_preds_np, temp_targets_np)
                        
                        if temp_f1 > best_threshold_f1:
                            best_threshold_f1 = temp_f1
                            best_preds = temp_preds_np
                    
                    if best_preds is not None:
                        all_preds.append(best_preds)
                        all_targets.append(targets.cpu().numpy())
                    
                except Exception as e:
                    print(f"âš ï¸  æ¤œè¨¼ãƒãƒƒãƒã‚¨ãƒ©ãƒ¼: {e}")
                    continue
        
        if all_preds and all_targets:
            all_preds = np.concatenate(all_preds, axis=0)
            all_targets = np.concatenate(all_targets, axis=0)
            precision, recall, f1 = compute_precision_recall_f1_per_class(all_preds, all_targets)
            
            print(f"ğŸ“Š [Epoch {epoch+1}] Val Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}")
            
            # ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜
            if f1 > best_f1:
                best_f1 = f1
                best_model_state = model.state_dict().copy()
                print(f"ğŸŒŸ æ–°ã—ã„ãƒ™ã‚¹ãƒˆF1ã‚¹ã‚³ã‚¢: {best_f1:.4f}")
        else:
            print(f"âŒ [Epoch {epoch+1}] æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã®å‡¦ç†ã«å¤±æ•—ã—ã¾ã—ãŸ")
    
    # ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜
    if best_model_state is not None:
        try:
            save_path = f"avion_best_f1_{best_f1:.4f}.pth"
            torch.save(best_model_state, save_path)
            print(f"ğŸ’¾ ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ã‚’ {save_path} ã«ä¿å­˜ã—ã¾ã—ãŸ (F1: {best_f1:.4f})")
        except Exception as e:
            print(f"âŒ ãƒ¢ãƒ‡ãƒ«ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
    
    print(f"\nğŸ¯ æœ€çµ‚çµæœ: ãƒ™ã‚¹ãƒˆF1ã‚¹ã‚³ã‚¢ = {best_f1:.4f}")
    if best_f1 >= 0.8:
        print("ğŸ‰ ç›®æ¨™ã®F1ã‚¹ã‚³ã‚¢0.8ä»¥ä¸Šã‚’é”æˆã—ã¾ã—ãŸï¼")
    else:
        print(f"ğŸ“ˆ ç›®æ¨™ã¾ã§ã‚ã¨{0.8 - best_f1:.4f}ãƒã‚¤ãƒ³ãƒˆã§ã™")

if __name__ == "__main__":
    main()