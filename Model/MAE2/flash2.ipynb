{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ollo/.cache/pypoetry/virtualenvs/videomae-clean-Ug0YGy1k-py3.11/lib/python3.11/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import torchvision.io as io\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from timm.models.layers import drop_path, to_2tuple, trunc_normal_\n",
    "\n",
    "\n",
    "def _cfg(url='', **kwargs):\n",
    "    return {\n",
    "        'url': url,\n",
    "        'num_classes': 400, 'input_size': (3, 224, 224), 'pool_size': None,\n",
    "        'crop_pct': .9, 'interpolation': 'bicubic',\n",
    "        'mean': (0.5, 0.5, 0.5), 'std': (0.5, 0.5, 0.5),\n",
    "        **kwargs\n",
    "    }\n",
    "\n",
    "class DropPath(nn.Module):\n",
    "    def __init__(self, drop_prob=None):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "    def forward(self, x):\n",
    "        return drop_path(x, self.drop_prob, self.training)\n",
    "\n",
    "class Mlp(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None,\n",
    "                 attn_drop=0., proj_drop=0., attn_head_dim=None, use_flash_attn=True):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.use_flash_attn = use_flash_attn\n",
    "        head_dim = dim // num_heads\n",
    "        if attn_head_dim is not None:\n",
    "            head_dim = attn_head_dim\n",
    "        self.head_dim = head_dim\n",
    "        self.all_head_dim = head_dim * self.num_heads\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "        self.qkv = nn.Linear(dim, self.all_head_dim * 3, bias=False)\n",
    "        if qkv_bias:\n",
    "            self.q_bias = nn.Parameter(torch.zeros(self.all_head_dim))\n",
    "            self.v_bias = nn.Parameter(torch.zeros(self.all_head_dim))\n",
    "        else:\n",
    "            self.q_bias = None\n",
    "            self.v_bias = None\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(self.all_head_dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        qkv_bias = None\n",
    "        if self.q_bias is not None:\n",
    "            qkv_bias = torch.cat((self.q_bias, torch.zeros_like(self.v_bias), self.v_bias))\n",
    "        qkv = F.linear(input=x, weight=self.qkv.weight, bias=qkv_bias)\n",
    "        qkv = qkv.reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        x = F.scaled_dot_product_attention(q, k, v,\n",
    "                                           dropout_p=0.0 if not self.training else self.attn_drop.p\n",
    "                                           ).transpose(1, 2).reshape(B, N, self.all_head_dim)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None,\n",
    "                 drop=0., attn_drop=0., drop_path=0., init_values=None,\n",
    "                 act_layer=nn.GELU, norm_layer=nn.LayerNorm,\n",
    "                 attn_head_dim=None, use_flash_attn=True):\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = Attention(dim, num_heads, qkv_bias, qk_scale, attn_drop,\n",
    "                              drop, attn_head_dim, use_flash_attn)\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(dim, mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
    "        if init_values > 0:\n",
    "            self.gamma_1 = nn.Parameter(init_values * torch.ones(dim))\n",
    "            self.gamma_2 = nn.Parameter(init_values * torch.ones(dim))\n",
    "        else:\n",
    "            self.gamma_1 = self.gamma_2 = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.gamma_1 is None:\n",
    "            x = x + self.drop_path(self.attn(self.norm1(x)))\n",
    "            x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "        else:\n",
    "            x = x + self.drop_path(self.gamma_1 * self.attn(self.norm1(x)))\n",
    "            x = x + self.drop_path(self.gamma_2 * self.mlp(self.norm2(x)))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbed(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=16, in_chans=3,\n",
    "                 embed_dim=768, num_frames=16, tubelet_size=2):\n",
    "        super().__init__()\n",
    "        img_size = to_2tuple(img_size)\n",
    "        patch_size = to_2tuple(patch_size)\n",
    "        self.tubelet_size = tubelet_size\n",
    "        num_patches = (img_size[0] // patch_size[0]) * (img_size[1] // patch_size[1]) * (num_frames // tubelet_size)\n",
    "        self.num_patches = num_patches\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.proj = nn.Conv3d(in_chans, embed_dim,\n",
    "                              kernel_size=(tubelet_size, patch_size[0], patch_size[1]),\n",
    "                              stride=(tubelet_size, patch_size[0], patch_size[1]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, T, H, W = x.shape\n",
    "        x = self.proj(x).flatten(2).transpose(1, 2)  # (B, N, D)\n",
    "        return x\n",
    "\n",
    "def get_sinusoid_encoding_table(n_position, d_hid):\n",
    "    def get_angle(pos, i):\n",
    "        return pos / np.power(10000, 2 * (i // 2) / d_hid)\n",
    "    table = np.array([[get_angle(pos, i) for i in range(d_hid)] for pos in range(n_position)])\n",
    "    table[:, 0::2] = np.sin(table[:, 0::2])\n",
    "    table[:, 1::2] = np.cos(table[:, 1::2])\n",
    "    return torch.tensor(table, dtype=torch.float).unsqueeze(0)  # (1, n_position, d_hid)\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=16, in_chans=3,\n",
    "                 num_classes=58, embed_dim=768, depth=12, num_heads=12,\n",
    "                 mlp_ratio=4., qkv_bias=True, qk_scale=None,\n",
    "                 fc_drop_rate=0., drop_rate=0., attn_drop_rate=0.,\n",
    "                 drop_path_rate=0., norm_layer=nn.LayerNorm,\n",
    "                 init_values=0., use_learnable_pos_emb=False,\n",
    "                 init_scale=0., all_frames=16, tubelet_size=2,\n",
    "                 use_checkpoint=False, use_mean_pooling=True,\n",
    "                 use_flash_attn=True):\n",
    "        super().__init__()\n",
    "        self.patch_embed = PatchEmbed(img_size, patch_size, in_chans,\n",
    "                                      embed_dim, all_frames, tubelet_size)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim)) if use_learnable_pos_emb \\\n",
    "                         else get_sinusoid_encoding_table(num_patches, embed_dim)\n",
    "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
    "\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]\n",
    "        self.blocks = nn.ModuleList([\n",
    "            Block(embed_dim, num_heads, mlp_ratio, qkv_bias, qk_scale,\n",
    "                  drop_rate, attn_drop_rate, dpr[i], init_values,\n",
    "                  norm_layer=norm_layer, act_layer=nn.GELU,\n",
    "                  use_flash_attn=use_flash_attn)\n",
    "            for i in range(depth)\n",
    "        ])\n",
    "        self.norm = nn.Identity() if use_mean_pooling else norm_layer(embed_dim)\n",
    "        self.fc_norm = norm_layer(embed_dim) if use_mean_pooling else None\n",
    "        self.fc_dropout = nn.Dropout(fc_drop_rate) if fc_drop_rate > 0 else nn.Identity()\n",
    "        self.head = nn.Linear(embed_dim, num_classes)\n",
    "        trunc_normal_(self.head.weight, std=.02)\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        x = self.patch_embed(x)\n",
    "        x = x + self.pos_embed.to(x.device)\n",
    "        x = self.pos_drop(x)\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "        x = self.norm(x)\n",
    "        return self.fc_norm(x.mean(1)) if self.fc_norm else x[:, 0]\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.head(self.fc_dropout(self.forward_features(x)))\n",
    "\n",
    "def get_model():\n",
    "    return VisionTransformer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mEgo4DFlashDataset\u001b[39;00m(\u001b[43mDataset\u001b[49m):\n\u001b[32m      2\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, annotation_file, video_root, transform=\u001b[38;5;28;01mNone\u001b[39;00m, num_frames=\u001b[32m16\u001b[39m, num_classes=\u001b[32m58\u001b[39m):\n\u001b[32m      3\u001b[39m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(annotation_file, \u001b[33m\"\u001b[39m\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[31mNameError\u001b[39m: name 'Dataset' is not defined"
     ]
    }
   ],
   "source": [
    "class Ego4DFlashDataset(Dataset):\n",
    "    def __init__(self, annotation_file, video_root, transform=None, num_frames=16, num_classes=58):\n",
    "        with open(annotation_file, \"r\") as f:\n",
    "            data = json.load(f)\n",
    "        self.annotations = data[\"annotations\"]\n",
    "        self.video_root = video_root\n",
    "        self.transform = transform\n",
    "        self.num_frames = num_frames\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ann = self.annotations[idx]\n",
    "        video_path = os.path.join(self.video_root, ann[\"video_url\"])\n",
    "        video, _, _ = io.read_video(video_path, pts_unit='sec')\n",
    "        T = video.shape[0]\n",
    "        if T < self.num_frames:\n",
    "            repeat_factor = (self.num_frames + T - 1) // T\n",
    "            video = video.repeat(repeat_factor, 1, 1, 1)\n",
    "        indices = torch.linspace(0, T - 1, self.num_frames).long()\n",
    "        video = video[indices].permute(0, 3, 1, 2).float() / 255.0\n",
    "        if self.transform:\n",
    "            video = torch.stack([self.transform(frame) for frame in video])\n",
    "        video = video.permute(1, 0, 2, 3)  # (C, T, H, W)\n",
    "        label = ann[\"label\"]\n",
    "        if not isinstance(label, list):\n",
    "            label = [label]\n",
    "        target = torch.zeros(self.num_classes)\n",
    "        for l in label:\n",
    "            if 0 <= l < self.num_classes:\n",
    "                target[l] = 1.0\n",
    "        return video, target\n",
    "\n",
    "def train_model_interactive():\n",
    "    annotation_dir = \"/home/ollo/videomae-clean\"\n",
    "    video_root = \"/srv/shared/data/ego4d/short_clips/verb_annotation_simple\"\n",
    "    checkpoint_path = \"/home/ollo/VideoMAE/checkpoints/vit_b_hybrid_pt_800e_k710_ft.pth\"\n",
    "    train_json = os.path.join(annotation_dir, \"20250512_annotations_train.json\")\n",
    "    val_json = os.path.join(annotation_dir, \"20250512_annotations_val.json\")\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "    ])\n",
    "\n",
    "    train_dataset = Ego4DFlashDataset(train_json, video_root, transform)\n",
    "    val_dataset = Ego4DFlashDataset(val_json, video_root, transform)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, num_workers=0)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=2, shuffle=False, num_workers=0)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = get_model().to(device)\n",
    "\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        checkpoint_data = torch.load(checkpoint_path, map_location=device)\n",
    "        model.load_state_dict(checkpoint_data, strict=False)\n",
    "        print(f\"Loaded checkpoint from {checkpoint_path}\")\n",
    "    else:\n",
    "        print(f\"Checkpoint not found at {checkpoint_path}. Starting from scratch.\")\n",
    "\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=1e-4)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    scaler = GradScaler()\n",
    "\n",
    "    for epoch in range(1):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for videos, targets in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
    "            videos, targets = videos.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            with autocast():\n",
    "                outputs = model(videos)\n",
    "                loss = criterion(outputs, targets)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")\n",
    "\n",
    "        model.eval()\n",
    "        correct = total = 0\n",
    "        with torch.no_grad():\n",
    "            for videos, targets in val_loader:\n",
    "                videos, targets = videos.to(device), targets.to(device)\n",
    "                with autocast():\n",
    "                    outputs = model(videos)\n",
    "                preds = (torch.sigmoid(outputs) > 0.5).float()\n",
    "                correct += (preds == targets).sum().item()\n",
    "                total += targets.numel()\n",
    "        accuracy = 100 * correct / total if total > 0 else 0\n",
    "        print(f\"Validation Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "    torch.save(model.state_dict(), \"videomae_finetuned_interactive.pth\")\n",
    "    print(\"Training complete. Model saved to 'videomae_finetuned_interactive.pth'.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_model_interactive()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (videomae-clean)",
   "language": "python",
   "name": "videomae-clean"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
