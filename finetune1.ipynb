{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Checkpoint を読み込み中...\n",
      "✅ 読み込み成功。キー: ['module']\n",
      "🔧 モデル構築中...\n",
      "✅ モデル構築完了\n",
      "⚠️ head.weight を削除\n",
      "⚠️ head.bias を削除\n",
      "✅ 重み読み込み完了\n",
      "  missing_keys: ['head.weight', 'head.bias']\n",
      "  unexpected_keys: []\n",
      "patch_embed.proj.weight: 差分 0.0\n",
      "patch_embed.proj.bias: 差分 0.0\n",
      "blocks.0.norm1.weight: 差分 0.0\n",
      "blocks.0.norm1.bias: 差分 0.0\n",
      "blocks.0.attn.q_bias: 差分 0.0\n",
      "blocks.0.attn.v_bias: 差分 0.0\n",
      "blocks.0.attn.qkv.weight: 差分 0.0\n",
      "blocks.0.attn.proj.weight: 差分 0.0\n",
      "blocks.0.attn.proj.bias: 差分 0.0\n",
      "blocks.0.norm2.weight: 差分 0.0\n",
      "blocks.0.norm2.bias: 差分 0.0\n",
      "blocks.0.mlp.fc1.weight: 差分 0.0\n",
      "blocks.0.mlp.fc1.bias: 差分 0.0\n",
      "blocks.0.mlp.fc2.weight: 差分 0.0\n",
      "blocks.0.mlp.fc2.bias: 差分 0.0\n",
      "blocks.1.norm1.weight: 差分 0.0\n",
      "blocks.1.norm1.bias: 差分 0.0\n",
      "blocks.1.attn.q_bias: 差分 0.0\n",
      "blocks.1.attn.v_bias: 差分 0.0\n",
      "blocks.1.attn.qkv.weight: 差分 0.0\n",
      "blocks.1.attn.proj.weight: 差分 0.0\n",
      "blocks.1.attn.proj.bias: 差分 0.0\n",
      "blocks.1.norm2.weight: 差分 0.0\n",
      "blocks.1.norm2.bias: 差分 0.0\n",
      "blocks.1.mlp.fc1.weight: 差分 0.0\n",
      "blocks.1.mlp.fc1.bias: 差分 0.0\n",
      "blocks.1.mlp.fc2.weight: 差分 0.0\n",
      "blocks.1.mlp.fc2.bias: 差分 0.0\n",
      "blocks.2.norm1.weight: 差分 0.0\n",
      "blocks.2.norm1.bias: 差分 0.0\n",
      "blocks.2.attn.q_bias: 差分 0.0\n",
      "blocks.2.attn.v_bias: 差分 0.0\n",
      "blocks.2.attn.qkv.weight: 差分 0.0\n",
      "blocks.2.attn.proj.weight: 差分 0.0\n",
      "blocks.2.attn.proj.bias: 差分 0.0\n",
      "blocks.2.norm2.weight: 差分 0.0\n",
      "blocks.2.norm2.bias: 差分 0.0\n",
      "blocks.2.mlp.fc1.weight: 差分 0.0\n",
      "blocks.2.mlp.fc1.bias: 差分 0.0\n",
      "blocks.2.mlp.fc2.weight: 差分 0.0\n",
      "blocks.2.mlp.fc2.bias: 差分 0.0\n",
      "blocks.3.norm1.weight: 差分 0.0\n",
      "blocks.3.norm1.bias: 差分 0.0\n",
      "blocks.3.attn.q_bias: 差分 0.0\n",
      "blocks.3.attn.v_bias: 差分 0.0\n",
      "blocks.3.attn.qkv.weight: 差分 0.0\n",
      "blocks.3.attn.proj.weight: 差分 0.0\n",
      "blocks.3.attn.proj.bias: 差分 0.0\n",
      "blocks.3.norm2.weight: 差分 0.0\n",
      "blocks.3.norm2.bias: 差分 0.0\n",
      "blocks.3.mlp.fc1.weight: 差分 0.0\n",
      "blocks.3.mlp.fc1.bias: 差分 0.0\n",
      "blocks.3.mlp.fc2.weight: 差分 0.0\n",
      "blocks.3.mlp.fc2.bias: 差分 0.0\n",
      "blocks.4.norm1.weight: 差分 0.0\n",
      "blocks.4.norm1.bias: 差分 0.0\n",
      "blocks.4.attn.q_bias: 差分 0.0\n",
      "blocks.4.attn.v_bias: 差分 0.0\n",
      "blocks.4.attn.qkv.weight: 差分 0.0\n",
      "blocks.4.attn.proj.weight: 差分 0.0\n",
      "blocks.4.attn.proj.bias: 差分 0.0\n",
      "blocks.4.norm2.weight: 差分 0.0\n",
      "blocks.4.norm2.bias: 差分 0.0\n",
      "blocks.4.mlp.fc1.weight: 差分 0.0\n",
      "blocks.4.mlp.fc1.bias: 差分 0.0\n",
      "blocks.4.mlp.fc2.weight: 差分 0.0\n",
      "blocks.4.mlp.fc2.bias: 差分 0.0\n",
      "blocks.5.norm1.weight: 差分 0.0\n",
      "blocks.5.norm1.bias: 差分 0.0\n",
      "blocks.5.attn.q_bias: 差分 0.0\n",
      "blocks.5.attn.v_bias: 差分 0.0\n",
      "blocks.5.attn.qkv.weight: 差分 0.0\n",
      "blocks.5.attn.proj.weight: 差分 0.0\n",
      "blocks.5.attn.proj.bias: 差分 0.0\n",
      "blocks.5.norm2.weight: 差分 0.0\n",
      "blocks.5.norm2.bias: 差分 0.0\n",
      "blocks.5.mlp.fc1.weight: 差分 0.0\n",
      "blocks.5.mlp.fc1.bias: 差分 0.0\n",
      "blocks.5.mlp.fc2.weight: 差分 0.0\n",
      "blocks.5.mlp.fc2.bias: 差分 0.0\n",
      "blocks.6.norm1.weight: 差分 0.0\n",
      "blocks.6.norm1.bias: 差分 0.0\n",
      "blocks.6.attn.q_bias: 差分 0.0\n",
      "blocks.6.attn.v_bias: 差分 0.0\n",
      "blocks.6.attn.qkv.weight: 差分 0.0\n",
      "blocks.6.attn.proj.weight: 差分 0.0\n",
      "blocks.6.attn.proj.bias: 差分 0.0\n",
      "blocks.6.norm2.weight: 差分 0.0\n",
      "blocks.6.norm2.bias: 差分 0.0\n",
      "blocks.6.mlp.fc1.weight: 差分 0.0\n",
      "blocks.6.mlp.fc1.bias: 差分 0.0\n",
      "blocks.6.mlp.fc2.weight: 差分 0.0\n",
      "blocks.6.mlp.fc2.bias: 差分 0.0\n",
      "blocks.7.norm1.weight: 差分 0.0\n",
      "blocks.7.norm1.bias: 差分 0.0\n",
      "blocks.7.attn.q_bias: 差分 0.0\n",
      "blocks.7.attn.v_bias: 差分 0.0\n",
      "blocks.7.attn.qkv.weight: 差分 0.0\n",
      "blocks.7.attn.proj.weight: 差分 0.0\n",
      "blocks.7.attn.proj.bias: 差分 0.0\n",
      "blocks.7.norm2.weight: 差分 0.0\n",
      "blocks.7.norm2.bias: 差分 0.0\n",
      "blocks.7.mlp.fc1.weight: 差分 0.0\n",
      "blocks.7.mlp.fc1.bias: 差分 0.0\n",
      "blocks.7.mlp.fc2.weight: 差分 0.0\n",
      "blocks.7.mlp.fc2.bias: 差分 0.0\n",
      "blocks.8.norm1.weight: 差分 0.0\n",
      "blocks.8.norm1.bias: 差分 0.0\n",
      "blocks.8.attn.q_bias: 差分 0.0\n",
      "blocks.8.attn.v_bias: 差分 0.0\n",
      "blocks.8.attn.qkv.weight: 差分 0.0\n",
      "blocks.8.attn.proj.weight: 差分 0.0\n",
      "blocks.8.attn.proj.bias: 差分 0.0\n",
      "blocks.8.norm2.weight: 差分 0.0\n",
      "blocks.8.norm2.bias: 差分 0.0\n",
      "blocks.8.mlp.fc1.weight: 差分 0.0\n",
      "blocks.8.mlp.fc1.bias: 差分 0.0\n",
      "blocks.8.mlp.fc2.weight: 差分 0.0\n",
      "blocks.8.mlp.fc2.bias: 差分 0.0\n",
      "blocks.9.norm1.weight: 差分 0.0\n",
      "blocks.9.norm1.bias: 差分 0.0\n",
      "blocks.9.attn.q_bias: 差分 0.0\n",
      "blocks.9.attn.v_bias: 差分 0.0\n",
      "blocks.9.attn.qkv.weight: 差分 0.0\n",
      "blocks.9.attn.proj.weight: 差分 0.0\n",
      "blocks.9.attn.proj.bias: 差分 0.0\n",
      "blocks.9.norm2.weight: 差分 0.0\n",
      "blocks.9.norm2.bias: 差分 0.0\n",
      "blocks.9.mlp.fc1.weight: 差分 0.0\n",
      "blocks.9.mlp.fc1.bias: 差分 0.0\n",
      "blocks.9.mlp.fc2.weight: 差分 0.0\n",
      "blocks.9.mlp.fc2.bias: 差分 0.0\n",
      "blocks.10.norm1.weight: 差分 0.0\n",
      "blocks.10.norm1.bias: 差分 0.0\n",
      "blocks.10.attn.q_bias: 差分 0.0\n",
      "blocks.10.attn.v_bias: 差分 0.0\n",
      "blocks.10.attn.qkv.weight: 差分 0.0\n",
      "blocks.10.attn.proj.weight: 差分 0.0\n",
      "blocks.10.attn.proj.bias: 差分 0.0\n",
      "blocks.10.norm2.weight: 差分 0.0\n",
      "blocks.10.norm2.bias: 差分 0.0\n",
      "blocks.10.mlp.fc1.weight: 差分 0.0\n",
      "blocks.10.mlp.fc1.bias: 差分 0.0\n",
      "blocks.10.mlp.fc2.weight: 差分 0.0\n",
      "blocks.10.mlp.fc2.bias: 差分 0.0\n",
      "blocks.11.norm1.weight: 差分 0.0\n",
      "blocks.11.norm1.bias: 差分 0.0\n",
      "blocks.11.attn.q_bias: 差分 0.0\n",
      "blocks.11.attn.v_bias: 差分 0.0\n",
      "blocks.11.attn.qkv.weight: 差分 0.0\n",
      "blocks.11.attn.proj.weight: 差分 0.0\n",
      "blocks.11.attn.proj.bias: 差分 0.0\n",
      "blocks.11.norm2.weight: 差分 0.0\n",
      "blocks.11.norm2.bias: 差分 0.0\n",
      "blocks.11.mlp.fc1.weight: 差分 0.0\n",
      "blocks.11.mlp.fc1.bias: 差分 0.0\n",
      "blocks.11.mlp.fc2.weight: 差分 0.0\n",
      "blocks.11.mlp.fc2.bias: 差分 0.0\n",
      "fc_norm.weight: 差分 0.0\n",
      "fc_norm.bias: 差分 0.0\n",
      "✅ CPU forward 成功 → 出力 shape: torch.Size([1, 57])\n",
      "❌ model.to(cuda) 失敗: CUDA error: device-side assert triggered\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import json\n",
    "import cv2\n",
    "\n",
    "# CUDA同期エラー出力を有効化（重要）\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "# ✅ パス追加（Jupyter用なら明示的に）\n",
    "# source_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n",
    "source_dir = \"/home/ollo/VideoMAE\"  # あなたの環境に応じて調整\n",
    "sys.path.append(source_dir)\n",
    "\n",
    "from modeling_finetune import vit_base_patch16_224\n",
    "\n",
    "\n",
    "def load_model():\n",
    "    checkpoint_path = \"/home/ollo/VideoMAE/checkpoints/vit_b_hybrid_pt_800e_k710_ft.pth\"\n",
    "\n",
    "    print(\"🔍 Checkpoint を読み込み中...\")\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=\"cpu\")\n",
    "    print(f\"✅ 読み込み成功。キー: {list(checkpoint.keys())}\")\n",
    "\n",
    "    state_dict = checkpoint[\"module\"]\n",
    "\n",
    "    print(\"🔧 モデル構築中...\")\n",
    "    model = vit_base_patch16_224(\n",
    "        all_frames=16,\n",
    "        img_size=224,\n",
    "        use_checkpoint=True,\n",
    "        num_classes=57\n",
    "    )\n",
    "    print(\"✅ モデル構築完了\")\n",
    "\n",
    "    # 出力層（head）の重みが不要なので削除\n",
    "    for del_key in [\"head.weight\", \"head.bias\"]:\n",
    "        if del_key in state_dict:\n",
    "            del state_dict[del_key]\n",
    "            print(f\"⚠️ {del_key} を削除\")\n",
    "\n",
    "    missing_keys, unexpected_keys = model.load_state_dict(state_dict, strict=False)\n",
    "    print(\"✅ 重み読み込み完了\")\n",
    "    print(\"  missing_keys:\", missing_keys)\n",
    "    print(\"  unexpected_keys:\", unexpected_keys)\n",
    "\n",
    "    # 重みの差分チェック（任意）\n",
    "    for key in model.state_dict():\n",
    "        if \"head\" in key:\n",
    "            continue\n",
    "        try:\n",
    "            diff = (model.state_dict()[key] - state_dict[key]).abs().sum()\n",
    "            print(f\"{key}: 差分 {diff}\")\n",
    "        except Exception:\n",
    "            print(f\"{key}: 差分計算スキップ（初期化済み）\")\n",
    "\n",
    "    # ✅ CPUで forward 確認してから GPU に送る\n",
    "    model.eval()\n",
    "    dummy = torch.randn(1, 3, 16, 224, 224)\n",
    "    try:\n",
    "        out = model(dummy)\n",
    "        print(f\"✅ CPU forward 成功 → 出力 shape: {out.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ CPU forward 失敗: {e}\")\n",
    "        return\n",
    "\n",
    "    # ✅ GPU に安全に送る（壊れた重みがないかチェック）\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    try:\n",
    "        model.to(device)\n",
    "        print(f\"✅ model.to({device}) 成功\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ model.to({device}) 失敗: {e}\")\n",
    "        return\n",
    "\n",
    "    # ✅ 最終テスト：GPU上で forward/backward\n",
    "    _input = torch.randn(1, 3, 16, 224, 224).to(device)\n",
    "    try:\n",
    "        _output = model(_input)\n",
    "        loss = _output.sum()\n",
    "        loss.backward()\n",
    "        print(\"✅ GPU上で forward/backward 成功\")\n",
    "        print(f\"出力 shape: {_output.shape}\")\n",
    "    except Exception as e:\n",
    "        print(\"❌ GPU上で forward/backward 失敗:\", e)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    load_model()\n",
    "    # check_annotation_video()  # ← 必要な時だけ有効化\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 136\u001b[39m\n\u001b[32m    134\u001b[39m \u001b[38;5;66;03m# ✅ 実行\u001b[39;00m\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m     \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 92\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     89\u001b[39m model.load_state_dict(state_dict, strict=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     91\u001b[39m device = torch.device(\u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     94\u001b[39m optimizer = optim.AdamW(model.parameters(), lr=\u001b[32m1e-4\u001b[39m, weight_decay=\u001b[32m0.05\u001b[39m)\n\u001b[32m     95\u001b[39m criterion = nn.CrossEntropyLoss()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/videomae-clean-Ug0YGy1k-py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1173\u001b[39m, in \u001b[36mModule.to\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1170\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1171\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1173\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/videomae-clean-Ug0YGy1k-py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:779\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    777\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    778\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m779\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    781\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[32m    782\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    783\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    784\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    789\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    790\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/videomae-clean-Ug0YGy1k-py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:779\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    777\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    778\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m779\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    781\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[32m    782\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    783\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    784\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    789\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    790\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/videomae-clean-Ug0YGy1k-py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:804\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    800\u001b[39m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[32m    801\u001b[39m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[32m    802\u001b[39m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[32m    803\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m804\u001b[39m     param_applied = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    805\u001b[39m p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n\u001b[32m    807\u001b[39m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/videomae-clean-Ug0YGy1k-py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1159\u001b[39m, in \u001b[36mModule.to.<locals>.convert\u001b[39m\u001b[34m(t)\u001b[39m\n\u001b[32m   1152\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t.dim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[32m4\u001b[39m, \u001b[32m5\u001b[39m):\n\u001b[32m   1153\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m t.to(\n\u001b[32m   1154\u001b[39m             device,\n\u001b[32m   1155\u001b[39m             dtype \u001b[38;5;28;01mif\u001b[39;00m t.is_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t.is_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1156\u001b[39m             non_blocking,\n\u001b[32m   1157\u001b[39m             memory_format=convert_to_format,\n\u001b[32m   1158\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1159\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1160\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1161\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1162\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1163\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1164\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1165\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) == \u001b[33m\"\u001b[39m\u001b[33mCannot copy out of meta tensor; no data!\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[31mRuntimeError\u001b[39m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import torch\n",
    "import cv2\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# CUDA同期エラー特定（重要）\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "# VideoMAE モデルのパスを追加\n",
    "sys.path.append(\"/home/ollo/VideoMAE\")\n",
    "from modeling_finetune import vit_base_patch16_224\n",
    "\n",
    "# ✅ Ego4D Dataset\n",
    "class Ego4DDataset(Dataset):\n",
    "    def __init__(self, annotation_file, video_root, transform=None, num_frames=16):\n",
    "        with open(annotation_file, \"r\") as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        self.annotations = data[\"annotations\"]\n",
    "        self.video_root = video_root\n",
    "        self.transform = transform\n",
    "        self.num_frames = num_frames\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.annotations[idx]\n",
    "        video_path = os.path.join(self.video_root, item[\"video_url\"])\n",
    "        label = item[\"label\"][0]  # 単一ラベル前提\n",
    "\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        frames = []\n",
    "\n",
    "        for i in range(self.num_frames):\n",
    "            frame_id = int(i * total_frames / self.num_frames)\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_id)\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            if self.transform:\n",
    "                frame = self.transform(frame)\n",
    "            frames.append(frame)\n",
    "        cap.release()\n",
    "\n",
    "        while len(frames) < self.num_frames:\n",
    "            frames.append(frames[-1])  # 最後のフレームで埋める\n",
    "\n",
    "        video_tensor = torch.stack(frames).permute(1, 0, 2, 3)  # [C, T, H, W]\n",
    "        return video_tensor, label\n",
    "\n",
    "# ✅ トレーニング関数\n",
    "def train_model():\n",
    "    # パス設定\n",
    "    annotation_dir = \"/home/ollo/videomae-clean\"\n",
    "    video_root = \"/srv/shared/data/ego4d/short_clips/verb_annotation_simple\"\n",
    "    checkpoint_path = \"/home/ollo/VideoMAE/checkpoints/vit_b_hybrid_pt_800e_k710_ft.pth\"\n",
    "\n",
    "    train_json = os.path.join(annotation_dir, \"20250512_annotations_train.json\")\n",
    "    val_json = os.path.join(annotation_dir, \"20250512_annotations_val.json\")\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    # DataLoader\n",
    "    train_dataset = Ego4DDataset(train_json, video_root, transform)\n",
    "    val_dataset = Ego4DDataset(val_json, video_root, transform)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=4)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, num_workers=4)\n",
    "\n",
    "    # モデル構築 + 重み読み込み\n",
    "    model = vit_base_patch16_224(all_frames=16, img_size=224, use_checkpoint=True, num_classes=57)\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=\"cpu\")\n",
    "    state_dict = checkpoint[\"module\"]\n",
    "    for k in [\"head.weight\", \"head.bias\"]:\n",
    "        if k in state_dict:\n",
    "            del state_dict[k]\n",
    "    model.load_state_dict(state_dict, strict=False)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.05)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    num_epochs = 5  # エポック数調整可能\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        for videos, labels in train_loader:\n",
    "            videos, labels = videos.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(videos)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"🔁 Epoch {epoch+1}/{num_epochs} | Train Loss: {total_loss:.4f}\")\n",
    "\n",
    "        # 検証\n",
    "        model.eval()\n",
    "        correct, total = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for videos, labels in val_loader:\n",
    "                videos, labels = videos.to(device), labels.to(device)\n",
    "                outputs = model(videos)\n",
    "                preds = torch.argmax(outputs, dim=1)\n",
    "                correct += (preds == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "\n",
    "        acc = 100.0 * correct / total\n",
    "        print(f\"✅ Val Accuracy: {acc:.2f}%\")\n",
    "\n",
    "    # モデル保存\n",
    "    save_path = os.path.join(annotation_dir, \"finetuned_model.pth\")\n",
    "    torch.save(model.state_dict(), save_path)\n",
    "    print(f\"💾 モデル保存完了: {save_path}\")\n",
    "\n",
    "# ✅ 実行\n",
    "if __name__ == \"__main__\":\n",
    "    train_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 missing keys: ['head.weight', 'head.bias']\n",
      "🔍 unexpected keys: []\n",
      "📦 Moving model to cuda...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 171\u001b[39m\n\u001b[32m    169\u001b[39m \u001b[38;5;66;03m# ✅ 実行\u001b[39;00m\n\u001b[32m    170\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m171\u001b[39m     \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 123\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    121\u001b[39m device = torch.device(\u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    122\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m📦 Moving model to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m123\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    125\u001b[39m optimizer = optim.AdamW(model.parameters(), lr=\u001b[32m1e-4\u001b[39m, weight_decay=\u001b[32m0.05\u001b[39m)\n\u001b[32m    126\u001b[39m criterion = nn.CrossEntropyLoss()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/videomae-clean-Ug0YGy1k-py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1173\u001b[39m, in \u001b[36mModule.to\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1170\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1171\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1173\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/videomae-clean-Ug0YGy1k-py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:779\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    777\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    778\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m779\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    781\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[32m    782\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    783\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    784\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    789\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    790\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/videomae-clean-Ug0YGy1k-py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:779\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    777\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    778\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m779\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    781\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[32m    782\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    783\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    784\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    789\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    790\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/videomae-clean-Ug0YGy1k-py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:804\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    800\u001b[39m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[32m    801\u001b[39m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[32m    802\u001b[39m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[32m    803\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m804\u001b[39m     param_applied = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    805\u001b[39m p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n\u001b[32m    807\u001b[39m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/videomae-clean-Ug0YGy1k-py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1159\u001b[39m, in \u001b[36mModule.to.<locals>.convert\u001b[39m\u001b[34m(t)\u001b[39m\n\u001b[32m   1152\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t.dim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[32m4\u001b[39m, \u001b[32m5\u001b[39m):\n\u001b[32m   1153\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m t.to(\n\u001b[32m   1154\u001b[39m             device,\n\u001b[32m   1155\u001b[39m             dtype \u001b[38;5;28;01mif\u001b[39;00m t.is_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t.is_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1156\u001b[39m             non_blocking,\n\u001b[32m   1157\u001b[39m             memory_format=convert_to_format,\n\u001b[32m   1158\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1159\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1160\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1161\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1162\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1163\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1164\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1165\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) == \u001b[33m\"\u001b[39m\u001b[33mCannot copy out of meta tensor; no data!\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[31mRuntimeError\u001b[39m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import torch\n",
    "import cv2\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# CUDA同期エラー特定（重要）\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "# VideoMAE モデルのパスを追加\n",
    "sys.path.append(\"/home/ollo/VideoMAE\")\n",
    "from modeling_finetune import vit_base_patch16_224\n",
    "\n",
    "# ✅ Ego4D Dataset\n",
    "class Ego4DDataset(Dataset):\n",
    "    def __init__(self, annotation_file, video_root, transform=None, num_frames=16):\n",
    "        with open(annotation_file, \"r\") as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        self.annotations = data[\"annotations\"]\n",
    "        self.video_root = video_root\n",
    "        self.transform = transform\n",
    "        self.num_frames = num_frames\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            item = self.annotations[idx]\n",
    "            video_path = os.path.join(self.video_root, item[\"video_url\"])\n",
    "\n",
    "            if not os.path.exists(video_path):\n",
    "                raise FileNotFoundError(f\"動画ファイルが見つかりません: {video_path}\")\n",
    "\n",
    "            label = int(item[\"label\"][0])\n",
    "            if not (0 <= label < 57):\n",
    "                raise ValueError(f\"❌ 無効なラベル値: {label}（範囲外）\")\n",
    "\n",
    "            cap = cv2.VideoCapture(video_path)\n",
    "            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "            frames = []\n",
    "\n",
    "            for i in range(self.num_frames):\n",
    "                frame_id = int(i * total_frames / self.num_frames)\n",
    "                cap.set(cv2.CAP_PROP_POS_FRAMES, frame_id)\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    print(f\"⚠️ フレーム取得失敗: {frame_id} @ {video_path}\")\n",
    "                    break\n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                if self.transform:\n",
    "                    frame = self.transform(frame)\n",
    "                frames.append(frame)\n",
    "            cap.release()\n",
    "\n",
    "            if not frames:\n",
    "                raise RuntimeError(f\"❌ フレームが取得できません: {video_path}\")\n",
    "\n",
    "            while len(frames) < self.num_frames:\n",
    "                frames.append(frames[-1])  # 最後のフレームで埋める\n",
    "\n",
    "            video_tensor = torch.stack(frames).permute(1, 0, 2, 3)  # [C, T, H, W]\n",
    "            return video_tensor, label\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ __getitem__ エラー at idx={idx}: {e}\")\n",
    "            raise\n",
    "\n",
    "# ✅ トレーニング関数\n",
    "def train_model():\n",
    "    # パス設定\n",
    "    annotation_dir = \"/home/ollo/videomae-clean\"\n",
    "    video_root = \"/srv/shared/data/ego4d/short_clips/verb_annotation_simple\"\n",
    "    checkpoint_path = \"/home/ollo/VideoMAE/checkpoints/vit_b_hybrid_pt_800e_k710_ft.pth\"\n",
    "\n",
    "    train_json = os.path.join(annotation_dir, \"20250512_annotations_train.json\")\n",
    "    val_json = os.path.join(annotation_dir, \"20250512_annotations_val.json\")\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    # DataLoader\n",
    "    train_dataset = Ego4DDataset(train_json, video_root, transform)\n",
    "    val_dataset = Ego4DDataset(val_json, video_root, transform)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=4)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, num_workers=4)\n",
    "\n",
    "    # ✅ モデル構築（num_classes=57）\n",
    "    model = vit_base_patch16_224(all_frames=16, img_size=224, use_checkpoint=True, num_classes=57)\n",
    "\n",
    "    # ✅ チェックポイント読み込み\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=\"cpu\")\n",
    "\n",
    "    # ✅ 柔軟に state_dict 抽出\n",
    "    if \"module\" in checkpoint:\n",
    "        state_dict = checkpoint[\"module\"]\n",
    "    elif \"model\" in checkpoint:\n",
    "        state_dict = checkpoint[\"model\"]\n",
    "    else:\n",
    "        state_dict = checkpoint\n",
    "\n",
    "    # ✅ head 層は読み込まず（ランダム初期化で再学習）\n",
    "    new_state_dict = {}\n",
    "    for k, v in state_dict.items():\n",
    "        if not k.startswith(\"head.\"):\n",
    "            new_state_dict[k] = v\n",
    "\n",
    "    missing_keys, unexpected_keys = model.load_state_dict(new_state_dict, strict=False)\n",
    "    print(\"🔍 missing keys:\", missing_keys)\n",
    "    print(\"🔍 unexpected keys:\", unexpected_keys)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"📦 Moving model to {device}...\")\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.05)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    num_epochs = 5\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        for videos, labels in train_loader:\n",
    "            videos, labels = videos.to(device), labels.to(device)\n",
    "\n",
    "            if labels.min() < 0 or labels.max() >= 57:\n",
    "                print(f\"❌ 不正なラベル検出: {labels}\")\n",
    "                continue\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(videos)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"🔁 Epoch {epoch+1}/{num_epochs} | Train Loss: {total_loss:.4f}\")\n",
    "\n",
    "        # 検証\n",
    "        model.eval()\n",
    "        correct, total = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for videos, labels in val_loader:\n",
    "                videos, labels = videos.to(device), labels.to(device)\n",
    "                outputs = model(videos)\n",
    "                preds = torch.argmax(outputs, dim=1)\n",
    "                correct += (preds == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "\n",
    "        acc = 100.0 * correct / total\n",
    "        print(f\"✅ Val Accuracy: {acc:.2f}%\")\n",
    "\n",
    "    # モデル保存\n",
    "    save_path = os.path.join(annotation_dir, \"finetuned_model.pth\")\n",
    "    torch.save(model.state_dict(), save_path)\n",
    "    print(f\"💾 モデル保存完了: {save_path}\")\n",
    "\n",
    "# ✅ 実行\n",
    "if __name__ == \"__main__\":\n",
    "    train_model()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (videomae-clean)",
   "language": "python",
   "name": "videomae-clean"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
