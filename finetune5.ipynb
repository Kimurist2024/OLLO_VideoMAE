{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: cannot access '/home/ollo/videomae-clean/serch/modeling_finetune.py': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "ls /home/ollo/videomae-clean/serch/modeling_finetune.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: av in /home/ollo/.cache/pypoetry/virtualenvs/videomae-clean-Ug0YGy1k-py3.11/lib/python3.11/site-packages (14.4.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install av\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PYTHONPATH„Å´ËøΩÂä†: /home/ollo/VideoMAE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ollo/.cache/pypoetry/virtualenvs/videomae-clean-Ug0YGy1k-py3.11/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/ollo/.cache/pypoetry/virtualenvs/videomae-clean-Ug0YGy1k-py3.11/lib/python3.11/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
      "/home/ollo/.cache/pypoetry/virtualenvs/videomae-clean-Ug0YGy1k-py3.11/lib/python3.11/site-packages/timm/models/registry.py:4: FutureWarning: Importing from timm.models.registry is deprecated, please import via timm.models\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.models\", FutureWarning)\n",
      "/home/ollo/VideoMAE/modeling_finetune.py:288: UserWarning: Overwriting vit_small_patch16_224 in registry with modeling_finetune.vit_small_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  @register_model\n",
      "/home/ollo/VideoMAE/modeling_finetune.py:297: UserWarning: Overwriting vit_base_patch16_224 in registry with modeling_finetune.vit_base_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  @register_model\n",
      "/home/ollo/VideoMAE/modeling_finetune.py:306: UserWarning: Overwriting vit_base_patch16_384 in registry with modeling_finetune.vit_base_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  @register_model\n",
      "/home/ollo/VideoMAE/modeling_finetune.py:315: UserWarning: Overwriting vit_large_patch16_224 in registry with modeling_finetune.vit_large_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  @register_model\n",
      "/home/ollo/VideoMAE/modeling_finetune.py:324: UserWarning: Overwriting vit_large_patch16_384 in registry with modeling_finetune.vit_large_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  @register_model\n",
      "üöÇ Epoch 1/5 [Train]:   0%|          | 0/16239 [00:00<?, ?it/s]/home/ollo/.cache/pypoetry/virtualenvs/videomae-clean-Ug0YGy1k-py3.11/lib/python3.11/site-packages/torch/nn/modules/conv.py:605: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return F.conv3d(\n",
      "/home/ollo/.cache/pypoetry/virtualenvs/videomae-clean-Ug0YGy1k-py3.11/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "                                                                                      \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 152\u001b[39m\n\u001b[32m    150\u001b[39m \u001b[38;5;66;03m# ‚úÖ ÂÆüË°å„Éñ„É≠„ÉÉ„ÇØ\u001b[39;00m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m152\u001b[39m     \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 120\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    117\u001b[39m     outputs = model(videos)\n\u001b[32m    118\u001b[39m     loss = criterion(outputs, targets)\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m \u001b[43mscaler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    121\u001b[39m scaler.step(optimizer)\n\u001b[32m    122\u001b[39m scaler.update()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/videomae-clean-Ug0YGy1k-py3.11/lib/python3.11/site-packages/torch/_tensor.py:525\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    515\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    516\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    517\u001b[39m         Tensor.backward,\n\u001b[32m    518\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    523\u001b[39m         inputs=inputs,\n\u001b[32m    524\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m525\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    526\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    527\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/videomae-clean-Ug0YGy1k-py3.11/lib/python3.11/site-packages/torch/autograd/__init__.py:267\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    262\u001b[39m     retain_graph = create_graph\n\u001b[32m    264\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    265\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    266\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m267\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    268\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    269\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    270\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    271\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    272\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    273\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    274\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    275\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/videomae-clean-Ug0YGy1k-py3.11/lib/python3.11/site-packages/torch/autograd/graph.py:744\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    742\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    743\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m744\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    745\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    746\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    747\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    748\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import torchvision.io as io\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ‚úÖ CUDA„Éá„Éê„ÉÉ„Ç∞Áî®Ë®≠ÂÆöÔºàÂøÖË¶Å„Å´Âøú„Åò„Å¶„Ç™„Éï„Å´„Åó„Å¶„Åè„Å†„Åï„ÅÑÔºâ\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "# ‚úÖ VideoMAE„ÅÆ„Éë„Çπ„Çísys.path„Å´ËøΩÂä†\n",
    "video_mae_path = '/home/ollo/VideoMAE'\n",
    "if video_mae_path not in sys.path:\n",
    "    sys.path.append(video_mae_path)\n",
    "    print(f\"PYTHONPATH„Å´ËøΩÂä†: {video_mae_path}\")\n",
    "\n",
    "# ‚úÖ VideoMAE„ÅÆ„É¢„Éá„É´„Çí„Ç§„É≥„Éù„Éº„Éà\n",
    "from modeling_finetune import vit_base_patch16_224\n",
    "\n",
    "# ‚úÖ Ego4DÁî® DatasetÔºàFlashAttentionÂØæÂøúÔºâ\n",
    "class Ego4DFlashDataset(Dataset):\n",
    "    def __init__(self, annotation_file, video_root, transform=None, num_frames=16, num_classes=58):\n",
    "        with open(annotation_file, \"r\") as f:\n",
    "            data = json.load(f)\n",
    "        self.annotations = data[\"annotations\"]\n",
    "        self.video_root = video_root\n",
    "        self.transform = transform\n",
    "        self.num_frames = num_frames\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ann = self.annotations[idx]\n",
    "        video_path = os.path.join(self.video_root, ann[\"video_url\"])\n",
    "        if not os.path.exists(video_path):\n",
    "            raise FileNotFoundError(f\"‚ùå ÂãïÁîª„Éï„Ç°„Ç§„É´„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åõ„Çì: {video_path}\")\n",
    "\n",
    "        video, _, _ = io.read_video(video_path, pts_unit='sec')  # [T, H, W, C]\n",
    "        T = video.shape[0]\n",
    "        if T < self.num_frames:\n",
    "            repeat_factor = (self.num_frames + T - 1) // T\n",
    "            video = video.repeat(repeat_factor, 1, 1, 1)\n",
    "\n",
    "        indices = torch.linspace(0, T - 1, self.num_frames).long()\n",
    "        video = video[indices]  # [T, H, W, C]\n",
    "        video = video.permute(0, 3, 1, 2).float() / 255.0  # [T, C, H, W]\n",
    "\n",
    "        if self.transform:\n",
    "            video = torch.stack([self.transform(frame) for frame in video])  # [T, C, H, W]\n",
    "\n",
    "        video = video.permute(1, 0, 2, 3)  # [C, T, H, W]\n",
    "\n",
    "        label = ann[\"label\"]\n",
    "        if not isinstance(label, list):\n",
    "            label = [label]\n",
    "        target = torch.zeros(self.num_classes)\n",
    "        for l in label:\n",
    "            if 0 <= l < self.num_classes:\n",
    "                target[l] = 1.0\n",
    "\n",
    "        return video, target\n",
    "\n",
    "# ‚úÖ „Éà„É¨„Éº„Éã„É≥„Ç∞Èñ¢Êï∞\n",
    "def train_model():\n",
    "    annotation_dir = \"/home/ollo/videomae-clean\"\n",
    "    video_root = \"/srv/shared/data/ego4d/short_clips/verb_annotation_simple\"\n",
    "    checkpoint_path = \"/home/ollo/VideoMAE/checkpoints/vit_b_hybrid_pt_800e_k710_ft.pth\"\n",
    "    train_json = os.path.join(annotation_dir, \"20250512_annotations_train.json\")\n",
    "    val_json = os.path.join(annotation_dir, \"20250512_annotations_val.json\")\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "    ])\n",
    "\n",
    "    train_dataset = Ego4DFlashDataset(train_json, video_root, transform)\n",
    "    val_dataset = Ego4DFlashDataset(val_json, video_root, transform)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=4, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = vit_base_patch16_224(\n",
    "        all_frames=16,\n",
    "        img_size=224,\n",
    "        use_checkpoint=True,\n",
    "        num_classes=58\n",
    "    ).to(device)\n",
    "\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=\"cpu\")\n",
    "    state_dict = checkpoint.get(\"module\") or checkpoint.get(\"model\") or checkpoint\n",
    "    new_state_dict = {k: v for k, v in state_dict.items() if not k.startswith(\"head.\")}\n",
    "    model.load_state_dict(new_state_dict, strict=False)\n",
    "\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.05)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    scaler = GradScaler()\n",
    "\n",
    "    num_epochs = 5\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        loop = tqdm(train_loader, desc=f\"üöÇ Epoch {epoch+1}/{num_epochs} [Train]\", leave=False)\n",
    "        for videos, targets in loop:\n",
    "            videos, targets = videos.to(device), targets.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            with autocast():\n",
    "                outputs = model(videos)\n",
    "                loss = criterion(outputs, targets)\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            loop.set_postfix(loss=loss.item())\n",
    "\n",
    "        print(f\"[Epoch {epoch+1}] üìâ Train Loss: {total_loss:.4f}\")\n",
    "\n",
    "        # Ê§úË®º\n",
    "        model.eval()\n",
    "        correct, total = 0, 0\n",
    "        loop = tqdm(val_loader, desc=f\"üß™ Epoch {epoch+1}/{num_epochs} [Val]\", leave=False)\n",
    "        with torch.no_grad():\n",
    "            for videos, targets in loop:\n",
    "                videos, targets = videos.to(device), targets.to(device)\n",
    "                with autocast():\n",
    "                    outputs = model(videos)\n",
    "                preds = (torch.sigmoid(outputs) > 0.5).float()\n",
    "                correct += (preds == targets).sum().item()\n",
    "                total += targets.numel()\n",
    "                loop.set_postfix(acc=f\"{100.0 * correct / total:.2f}%\")\n",
    "\n",
    "        acc = 100.0 * correct / total\n",
    "        print(f\"[Epoch {epoch+1}] ‚úÖ Val Accuracy: {acc:.2f}%\")\n",
    "\n",
    "    save_path = os.path.join(annotation_dir, \"videomae_finetuned_flash.pth\")\n",
    "    torch.save(model.state_dict(), save_path)\n",
    "    print(f\"üíæ „É¢„Éá„É´‰øùÂ≠òÂÆå‰∫Ü: {save_path}\")\n",
    "\n",
    "# ‚úÖ ÂÆüË°å„Éñ„É≠„ÉÉ„ÇØ\n",
    "if __name__ == \"__main__\":\n",
    "    train_model()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (videomae-clean)",
   "language": "python",
   "name": "videomae-clean"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
