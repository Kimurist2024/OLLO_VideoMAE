{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Trainファイル作成: /home/ollo/videomae-clean/20250512_annotations_train.json（64955件）\n",
      "✅ Valファイルコピー: /home/ollo/videomae-clean/20250512_annotations_val.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "def merge_train_splits(split_files, annotation_dir, output_dir):\n",
    "    merged_annotations = []\n",
    "    categories = None\n",
    "\n",
    "    for file_name in split_files:\n",
    "        path = os.path.join(annotation_dir, file_name)\n",
    "        with open(path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "            if categories is None:\n",
    "                categories = data.get(\"categories\", {})\n",
    "            merged_annotations.extend(data.get(\"annotations\", []))\n",
    "\n",
    "    merged_data = {\n",
    "        \"categories\": categories,\n",
    "        \"annotations\": merged_annotations\n",
    "    }\n",
    "\n",
    "    output_train = os.path.join(output_dir, \"20250512_annotations_train.json\")\n",
    "    with open(output_train, 'w') as f:\n",
    "        json.dump(merged_data, f, indent=2)\n",
    "\n",
    "    print(f\"✅ Trainファイル作成: {output_train}（{len(merged_annotations)}件）\")\n",
    "\n",
    "\n",
    "def copy_val_split(annotation_dir, output_dir):\n",
    "    src = os.path.join(annotation_dir, \"20250512_annotations_split_0.json\")\n",
    "    dst = os.path.join(output_dir, \"20250512_annotations_val.json\")\n",
    "    shutil.copyfile(src, dst)\n",
    "    print(f\"✅ Valファイルコピー: {dst}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    annotation_dir = \"/srv/shared/data/ego4d/annotations\"\n",
    "    output_dir = \"/home/ollo/videomae-clean\"\n",
    "\n",
    "    # 必要なら出力先を作成\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # train を split_1〜3 からマージ\n",
    "    split_files = [\n",
    "        \"20250512_annotations_split_1.json\",\n",
    "        \"20250512_annotations_split_2.json\",\n",
    "        \"20250512_annotations_split_3.json\"\n",
    "    ]\n",
    "    merge_train_splits(split_files, annotation_dir, output_dir)\n",
    "\n",
    "    # val を split_0 からコピー\n",
    "    copy_val_split(annotation_dir, output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0+cu118\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: psutil in /home/ollo/.cache/pypoetry/virtualenvs/videomae-clean-Ug0YGy1k-py3.11/lib/python3.11/site-packages (7.0.0)\n",
      "Collecting GPUtil\n",
      "  Downloading GPUtil-1.4.0.tar.gz (5.5 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: GPUtil\n",
      "\u001b[33m  DEPRECATION: Building 'GPUtil' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'GPUtil'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
      "\u001b[0m  Building wheel for GPUtil (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for GPUtil: filename=gputil-1.4.0-py3-none-any.whl size=7433 sha256=fc4fa0ca274f5513c03ca9cb8db3df8c9f99d660e6360b78f6132551945ca208\n",
      "  Stored in directory: /home/ollo/.cache/pip/wheels/2b/4d/8f/55fb4f7b9b591891e8d3f72977c4ec6c7763b39c19f0861595\n",
      "Successfully built GPUtil\n",
      "Installing collected packages: GPUtil\n",
      "Successfully installed GPUtil-1.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install psutil GPUtil\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== OS 情報 ===\n",
      "システム: Linux\n",
      "リリース: 5.10.0-34-cloud-amd64\n",
      "バージョン: #1 SMP Debian 5.10.234-1 (2025-02-24)\n",
      "\n",
      "=== CPU 情報 ===\n",
      "プロセッサ: \n",
      "物理コア数: 8\n",
      "論理コア数: 16\n",
      "\n",
      "=== メモリ情報 ===\n",
      "合計メモリ: 62.81 GB\n",
      "\n",
      "=== PyTorch / CUDA 情報 ===\n",
      "PyTorch バージョン: 2.3.0+cu118\n",
      "CUDA 利用可能: True\n",
      "CUDA バージョン: 11.8\n",
      "\n",
      "=== GPU 情報 ===\n",
      "GPU 名称: NVIDIA L4\n",
      "メモリ合計: 23034.0 MB\n",
      "使用率: 0.0%\n"
     ]
    }
   ],
   "source": [
    "import platform\n",
    "import psutil\n",
    "import torch\n",
    "import GPUtil\n",
    "\n",
    "# OS情報\n",
    "print(\"=== OS 情報 ===\")\n",
    "print(\"システム:\", platform.system())\n",
    "print(\"リリース:\", platform.release())\n",
    "print(\"バージョン:\", platform.version())\n",
    "\n",
    "# CPU情報\n",
    "print(\"\\n=== CPU 情報 ===\")\n",
    "print(\"プロセッサ:\", platform.processor())\n",
    "print(\"物理コア数:\", psutil.cpu_count(logical=False))\n",
    "print(\"論理コア数:\", psutil.cpu_count(logical=True))\n",
    "\n",
    "# メモリ情報\n",
    "print(\"\\n=== メモリ情報 ===\")\n",
    "mem = psutil.virtual_memory()\n",
    "print(f\"合計メモリ: {mem.total / 1024**3:.2f} GB\")\n",
    "\n",
    "# PyTorchバージョンとCUDA\n",
    "print(\"\\n=== PyTorch / CUDA 情報 ===\")\n",
    "print(\"PyTorch バージョン:\", torch.__version__)\n",
    "print(\"CUDA 利用可能:\", torch.cuda.is_available())\n",
    "print(\"CUDA バージョン:\", torch.version.cuda)\n",
    "\n",
    "# GPU情報（ある場合）\n",
    "print(\"\\n=== GPU 情報 ===\")\n",
    "if torch.cuda.is_available():\n",
    "    gpus = GPUtil.getGPUs()\n",
    "    for gpu in gpus:\n",
    "        print(f\"GPU 名称: {gpu.name}\")\n",
    "        print(f\"メモリ合計: {gpu.memoryTotal} MB\")\n",
    "        print(f\"使用率: {gpu.load*100:.1f}%\")\n",
    "else:\n",
    "    print(\"GPU は検出されませんでした。\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (videomae-clean)",
   "language": "python",
   "name": "videomae-clean"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
